import io
import json
import os
import re

import autogen
import hexagram_generator
import pandas as pd
import rag

llm_config = {
    "model": "gpt-4o",
    "api_key": os.environ["OPENAI_API_KEY"],
}


def termination_msg(x):
    return isinstance(x, dict) and "TERMINATE" == str(x.get("content", ""))[-9:].upper()


def fortune_telling(question, first_index, ri_gan, alter_list, return_interp=False):
    user = autogen.UserProxyAgent(
        name="User",
        code_execution_config=False,
        is_termination_msg=termination_msg,
        human_input_mode="NEVER",
        default_auto_reply="TERMINATE",
    )

    agent_configs = {
        "emotional": json.load(open("../agent_configs/emotional.json")),
        "rational": json.load(open("../agent_configs/rational.json")),
        "summary": json.load(open("../agent_configs/summary.json")),
    }
    hexagram_agent = autogen.AssistantAgent(
        name="Hexagram_Interpreter",
        system_message=open("../agent_configs/jiegua.md").read(),
        llm_config=llm_config,
    )
    emotional_agent = autogen.AssistantAgent(
        name=agent_configs["emotional"]["name"],
        system_message=agent_configs["emotional"]["system_message"],
        llm_config=llm_config,
    )

    rational_agent = autogen.AssistantAgent(
        name=agent_configs["rational"]["name"],
        system_message=open("../agent_configs/rational.md").read(),
        llm_config=llm_config,
    )

    summary_agent = autogen.AssistantAgent(
        name=agent_configs["summary"]["name"],
        system_message=agent_configs["summary"]["system_message"],
        llm_config=llm_config,
    )
    members = [user, hexagram_agent]
    if not return_interp:
        members.append(rational_agent)

    group_chat = autogen.GroupChat(
        agents=members,
        messages=[],
        max_round=6,
        speaker_selection_method="round_robin",
        allow_repeat_speaker=False,
    )

    manager = autogen.GroupChatManager(groupchat=group_chat, llm_config=llm_config)

    (
        process_time,
        first_index,
        first_name,
        first_details,
        alter_list,
        second_index,
        second_name,
        second_details,
    ) = hexagram_generator.manual_process(
        first_index=first_index, ri_gan=ri_gan, manual_list=alter_list
    )

    raw_gua_info = hexagram_generator.retrieve_information(
        first_index,
        second_index,
        alter_list,
        first_details,
        second_details,
    )
    formatted_gua_info, _ = hexagram_generator.format_gua_info(raw_gua_info)

    message_content = """
        Hello, masters. I came here to ask for my fortune.
        First, please allow me to provide you some examples.
        {example_text}
        Here's the information about the hexagram(s) I got:
        {hexagram_info}

        
**Here's my question: {question}**
    """
    example_text = rag.retrieve(question, 2)
    message = message_content.format(
        hexagram_info=formatted_gua_info, question=question, example_text=example_text
    )
    res = user.initiate_chat(
        manager,
        message=message,
    )
    return res


def marking(generated_answer, standard_answer, jiegua=False):
    answer_prompt = """
        You are the question marker. You need to quantatively evaluate the quality of the answer generated by a LLM Agent.
        It's the result of I Ching fortune-telling, so you need to evaluate base on how related the generated answer is to the question, and how it compares to the human expert answer.
        I'll give you the standard answer, and the generated answer you need to evaluate.
        We will have a full mark of 10.
        Here's the evaluation steps:
        1. You need to break down the standard answer into several key points, and assign a score to each point, based on their importance, and they sum up to 8. 
        2. If the generated answer is related to the question, you can give a score of 2.
        3. You need to evaluate the answer to see how many key points are covered. The score will be the sum of the scores of the key points covered.
        4. You need to provide a detailed explanation for each key point, to explain why it's a key point and how much score did you give at this bullet point for the generated answer.
        
        Your should finally output your evaluation in a csv table format, wrapping up with html label <table> and </table>. 
        Note that the relevance score will have a specific name 'Relevance' in the table.
        The separator should be a $ sign. 
        Example: 
        <table>

        Key Point$Full Score$Score
        He Loves me$2$1
        Be active$3$2
        any other key points$3$0
        Wordy$2$0
        

        </table>

        """
    jiegua_prompt = """
        You are the question marker. You need to quantatively evaluate the quality of the answer generated by a LLM Agent.
        It's the result of I Ching hexagram interpretation, so you need to evaluate base on how well did the generated interpretation follows the I Ching methodology, and how it compares to the human expert interpretation.
        I'll give you the human expert answer, and the LLM generated answer you need to evaluate.
        We will have a full mark of 10.
        Here's the evaluation steps:
        1. You need to break down the standard answer into several key points, and assign a score to each point, based on their importance, and they sum up to 10. 
        3. You need to evaluate the answer to see how many key points are covered. The score will be the sum of the scores of the key points covered.
        4. You need to provide a detailed explanation for each key point, to explain why it's a key point and how much score did you give at this bullet point for the generated answer.
        
        Your should finally output your evaluation in a csv table format, wrapping up with html label <table> and </table>.
        The separator should be a $ sign. 
        Example: 
        <table>

        Key Point$Full Score$Score
        He Loves me$2$1
        Be active$3$2
        any other key points$3$0
        

        </table>

        """
    score_agent = autogen.AssistantAgent(
        name="Score",
        system_message=answer_prompt if not jiegua else jiegua_prompt,
        llm_config=llm_config,
    )
    eval_user = autogen.UserProxyAgent(
        name="Eval_User",
        code_execution_config=False,
        is_termination_msg=termination_msg,
        human_input_mode="NEVER",
        default_auto_reply="TERMINATE",
    )
    evaluate_prompt = """
        Here's the standard answer:
        {standard_answer}
        Here's the generated answer:
        {generated_answer}
        Please evaluate the generated answer based on the standard answer.
    """
    evaluate_message = evaluate_prompt.format(
        standard_answer=standard_answer, generated_answer=generated_answer
    )
    eval_res = eval_user.initiate_chat(
        score_agent,
        message=evaluate_message,
    )
    text = eval_res.chat_history[-2]["content"]
    match = re.search(r"<table>(.*?)</table>", text, re.DOTALL)
    if match:
        table_text = match.group(1).strip()
        # Step 2: Read the CSV data into a pandas DataFrame
        df = pd.read_csv(io.StringIO(table_text), header=0, sep="$")
        df.columns = [x.strip() for x in df.columns]
        return df, text
    else:
        raise ValueError("No table found in the text.")
