import autogen
import os
import re
import pandas as pd
import io

llm_config = {
    "model": "gpt-4o",
    "api_key": os.environ["OPENAI_API_KEY"],
}


def termination_msg(x):
    return isinstance(x, dict) and "TERMINATE" == str(x.get("content", ""))[-9:].upper()


def marking(generated_answer, standard_answer):
    score_agent = autogen.AssistantAgent(
        name="Score",
        system_message="""
        You are the question marker. You need to quantatively evaluate the quality of the answer generated by a LLM Agent.
        It's the result of I Ching fortune-telling, so you need to evaluate base on how related the generated answer is to the question, and how it compares to the human expert answer.
        I'll give you the standard answer, and the generated answer you need to evaluate.
        We will have a full mark of 10.
        Here's the evaluation steps:
        1. You need to break down the standard answer into several key points, and assign a score to each point, based on their importance, and they sum up to 8. 
        2. You also need a score of 2 for how wordy the answer is, a wordy and vague answer will get lower score. All scores should sum up to 10.
        3. You need to evaluate the answer to see how many key points are covered. The score will be the sum of the scores of the key points covered.
        4. You need to provide a detailed explanation for each key point, to explain why it's a key point and how much score did you give at this bullet point for the generated answer.
        
        Your should finally output your evaluation in a csv table format, wrapping up with html label <table> and </table>. 
        Note that the wordy score will have a specific name 'Wordy' in the table.
        Example: 
        <table>

        Key Point,Full Score,Score
        He Loves me,2,1
        Be active,3,2
        any other key points,3,0
        Wordy,2,0
        

        </table>

        """,
        llm_config=llm_config,
    )
    eval_user = autogen.UserProxyAgent(
        name="Eval_User",
        code_execution_config=False,
        is_termination_msg=termination_msg,
        human_input_mode="NEVER",
        default_auto_reply="TERMINATE",
    )
    evaluate_prompt = """
        Here's the standard answer:
        {standard_answer}
        Here's the generated answer:
        {generated_answer}
        Please evaluate the generated answer based on the standard answer.
    """
    evaluate_message = evaluate_prompt.format(
        standard_answer=standard_answer, generated_answer=generated_answer
    )
    eval_res = eval_user.initiate_chat(
        score_agent,
        message=evaluate_message,
    )
    text = eval_res.chat_history[-2]["content"]
    match = re.search(r"<table>(.*?)</table>", text, re.DOTALL)
    if match:
        table_text = match.group(1).strip()
        # Step 2: Read the CSV data into a pandas DataFrame
        df = pd.read_csv(io.StringIO(table_text), header=0)
        return df
    else:
        raise ValueError("No table found in the text.")
